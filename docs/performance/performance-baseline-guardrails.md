# Performance Baseline Guardrails

This note tracks how we collect, retain, and interpret the performance baseline artifacts
generated by `npm run telemetry:performance:baseline`. The script aggregates five fresh
snapshots from `performanceSnapshot.js` and writes an aggregate JSON report that CI now
publishes alongside other telemetry.

## Latest Baseline (Session 86 bootstrap)

- **Artifact**: `telemetry-artifacts/performance/ci-baseline.json`
- **Generated at**: 2025-10-30T01:51:29.522Z
- **Runs aggregated**: 5

| Metric            | Avg (ms) | Threshold (ms) | Utilisation | Status   | Min / Max (ms) |
| ----------------- | -------- | -------------- | ----------- | -------- | -------------- |
| forensicAnalysis  | 0.0337   | 4              | 0.84%       | OK       | 0.0047 / 0.1467 |
| factionModify     | 0.003    | 2              | 0.15%       | OK       | 0.0013 / 0.0072 |
| factionAttitude   | 0.0002   | 0.05           | 0.40%       | OK       | 0 / 0.0007 |
| bspGeneration     | 5.4662   | 10             | 54.66%      | WARNING  | 3.1234 / 12.4603 |

All averages sit well below their thresholds; BSP generation remains the hottest path
and recorded a single spike over the 10 ms cap. The average still leaves >45% headroom,
but we should monitor BSP spikes to ensure they do not trend upwards. This snapshot serves as the reference
point for alerting thresholds described below.

## Alerting Rules

Our `summarizePerformanceBaseline.js` helper (see usage below) evaluates every metric and
raises warnings when utilisation crosses 80% of the configured threshold or when peak
samples exceed the cap. If utilisation exceeds 95% or `performanceBaseline.js` marks the
metric as failed, the helper escalates the status to `CRITICAL`.

Alert matrix:

| Utilisation vs. threshold | Status    | Action                                                                 |
| ------------------------- | --------- | ---------------------------------------------------------------------- |
| < 80%                     | OK/INFO   | No action required                                                     |
| 80% - 94.99%              | WARNING   | Investigate regressions, plan follow-up optimisation                   |
| >= 95% or baseline failure | CRITICAL  | Block merging until metric recovers, raise `PERF-119` follow-up issue  |

To surface alerts, run:

```bash
npm run telemetry:performance:baseline -- --runs 5 --out telemetry-artifacts/performance/ci-baseline.json
node scripts/telemetry/summarizePerformanceBaseline.js \
  --input telemetry-artifacts/performance/ci-baseline.json \
  --output docs/performance/performance-baseline-latest.md
```

The helper prints a markdown table and writes it to the optional `--output` path so the
latest baseline can be reviewed quickly in pull requests or stored with build artifacts.

## Retention Policy

1. After each CI run, copy the generated `ci-baseline.json` into
   `telemetry-artifacts/performance/history/<ISO-DATE>-ci-baseline.json`.
2. Keep the last **14** baseline aggregates (roughly two weeks of CI history) inside the
   `history/` folder; prune older files during weekly maintenance.
3. When a warning or critical alert fires, attach the corresponding JSON and markdown
   summary to the `PERF-119` backlog item notes so trend analysis remains centralised.

The `history/` directory stays git-ignored to avoid polluting the repository. CI uploads
the same files to artifact storage for long-term retention.

## Future Automation

- Wire `summarizePerformanceBaseline.js` into the CI workflow so every baseline job emits
  a markdown summary artifact next to the JSON aggregate.
- Extend the script with delta analysis against the previous baseline once the history
  directory contains more than one entry.
